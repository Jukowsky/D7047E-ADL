{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0af60ce-b191-4ea2-a364-c149a3c6e04d",
   "metadata": {},
   "source": [
    "Task 2 \n",
    "\n",
    "Run the same code with a different loss function: \n",
    "Logistic loss as described in Brandon Amos blog and compare the results with above Task 1. \n",
    "\n",
    "You may need to modify the network architectures slightly with logit loss. \n",
    "\n",
    "Run the code for 5, 10 and 50 epochs and observe the results in both cases. \n",
    "\n",
    "How/why is the output different for both\n",
    "cases? Try to find a suitable reason for both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a8fd08-4ea6-49ab-bc61-b9765827a1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcanatilgan\u001b[0m (\u001b[33mcanatilgan-lule-university-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/lab02_stefan/wandb/run-20250505_012345-cp3m6oea</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/cp3m6oea' target=\"_blank\">light-astromech-18</a></strong> to <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/cp3m6oea' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/cp3m6oea</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 71.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0; D_loss: 0.0780; G_loss: 5.6271\n",
      "Saved Best Models at epoch 0 | G_loss: 5.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1; D_loss: 0.0320; G_loss: 5.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 76.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2; D_loss: 0.0571; G_loss: 5.4029\n",
      "Saved Best Models at epoch 2 | G_loss: 5.4029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:09<00:00, 100.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3; D_loss: 0.0920; G_loss: 6.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4; D_loss: 0.1777; G_loss: 5.4179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 76.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5; D_loss: 0.3034; G_loss: 4.6860\n",
      "Saved Best Models at epoch 5 | G_loss: 4.6860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:11<00:00, 79.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6; D_loss: 0.4628; G_loss: 3.9318\n",
      "Saved Best Models at epoch 6 | G_loss: 3.9318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:09<00:00, 100.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7; D_loss: 0.5431; G_loss: 3.6702\n",
      "Saved Best Models at epoch 7 | G_loss: 3.6702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8; D_loss: 0.6244; G_loss: 3.2221\n",
      "Saved Best Models at epoch 8 | G_loss: 3.2221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9; D_loss: 0.7293; G_loss: 2.9135\n",
      "Saved Best Models at epoch 9 | G_loss: 2.9135\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "mb_size = 64\n",
    "Z_dim = 1000\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "\n",
    "# Load MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the 28x28 image to 784\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../MNIST', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=mb_size, shuffle=True)\n",
    "\n",
    "X_dim = 784  # 28 x 28\n",
    "\n",
    "# Xavier Initialization\n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, h_dim, x_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, x_dim)\n",
    "        self.apply(xavier_init)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        out = torch.sigmoid(self.fc2(h))\n",
    "        return out\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, 1)\n",
    "        self.apply(xavier_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        out = self.fc2(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "def logisticGANTraining(G, D, loss_fn, train_loader):\n",
    "    G.train()\n",
    "    D.train()\n",
    "\n",
    "    D_loss_real_total = 0\n",
    "    D_loss_fake_total = 0\n",
    "    G_loss_total = 0\n",
    "    t = tqdm.tqdm(train_loader)\n",
    "    \n",
    "    for it, (X_real, labels) in enumerate(t):\n",
    "        # Prepare real data\n",
    "        X_real = X_real.float().to(device)\n",
    "\n",
    "        # Sample noise and labels\n",
    "        z = torch.randn(X_real.size(0), Z_dim).to(device)\n",
    "        ones_label = torch.ones(X_real.size(0), 1).to(device)\n",
    "        zeros_label = torch.zeros(X_real.size(0), 1).to(device)\n",
    "\n",
    "        # ================= Train Discriminator =================\n",
    "        G_sample = G(z)\n",
    "        D_real = D(X_real)\n",
    "        D_fake = D(G_sample.detach())\n",
    "\n",
    "        D_loss_real = F.softplus(-D_real).mean()  # log(1 + e^{-D(x_real)})\n",
    "        D_loss_fake = F.softplus(D_fake).mean()   # log(1 + e^{D(x_fake)})\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        D_loss_real_total += D_loss_real.item()\n",
    "        D_loss_fake_total += D_loss_fake.item()\n",
    "\n",
    "        D_solver.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_solver.step()\n",
    "\n",
    "        # ================= Train Generator ====================\n",
    "        z = torch.randn(X_real.size(0), Z_dim).to(device)\n",
    "        G_sample = G(z)\n",
    "        D_fake = D(G_sample)\n",
    "\n",
    "        G_loss = F.softplus(-D_fake).mean()  # log(1 + e^{-D(G(z))})\n",
    "        G_loss_total += G_loss.item()\n",
    "\n",
    "        G_solver.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_solver.step()\n",
    "\n",
    "    # ================= Logging =================\n",
    "    D_loss_real_avg = D_loss_real_total / len(train_loader)\n",
    "    D_loss_fake_avg = D_loss_fake_total / len(train_loader)\n",
    "    D_loss_avg = D_loss_real_avg + D_loss_fake_avg\n",
    "    G_loss_avg = G_loss_total / len(train_loader)\n",
    "\n",
    "    wandb.log({\n",
    "        \"D_loss_real\": D_loss_real_avg,\n",
    "        \"D_loss_fake\": D_loss_fake_avg,\n",
    "        \"D_loss\": D_loss_avg,\n",
    "        \"G_loss\": G_loss_avg\n",
    "    })\n",
    "\n",
    "    return G, D, G_loss_avg, D_loss_avg\n",
    "    \n",
    "\n",
    "\n",
    "def save_sample(G, epoch, mb_size, Z_dim):\n",
    "    out_dir = \"out_vanila_GAN2\"\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(mb_size, Z_dim).to(device)\n",
    "        samples = G(z).detach().cpu().numpy()[:16]\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    if not os.path.exists(f'{out_dir}'):\n",
    "        os.makedirs(f'{out_dir}')\n",
    "\n",
    "    plt.savefig(f'{out_dir}/{str(epoch).zfill(3)}.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "########################### Main #######################################\n",
    "wandb_log = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate models\n",
    "G = Generator(Z_dim, h_dim, X_dim).to(device)\n",
    "D = Discriminator(X_dim, h_dim).to(device)\n",
    "\n",
    "# Optimizers\n",
    "G_solver = optim.Adam(G.parameters(), lr=lr)\n",
    "D_solver = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "#def my_bce_loss(preds, targets):\n",
    "#    return F.binary_cross_entropy(preds, targets)\n",
    "\n",
    "#loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn = None\n",
    "\n",
    "if wandb_log: \n",
    "    wandb.init(project=\"conditional-gan-mnist\")\n",
    "\n",
    "    # Log hyperparameters\n",
    "    wandb.config.update({\n",
    "        \"batch_size\": mb_size,\n",
    "        \"Z_dim\": Z_dim,\n",
    "        \"X_dim\": X_dim,\n",
    "        \"h_dim\": h_dim,\n",
    "        \"lr\": lr,\n",
    "    })\n",
    "\n",
    "best_g_loss = float('inf')  # Initialize best generator loss\n",
    "save_dir = 'checkpoints'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#Train epochs\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    G, D, G_loss_avg, D_loss_avg= logisticGANTraining(G, D, loss_fn, train_loader)\n",
    "\n",
    "    print(f'epoch{epoch}; D_loss: {D_loss_avg:.4f}; G_loss: {G_loss_avg:.4f}')\n",
    "\n",
    "    if G_loss_avg < best_g_loss:\n",
    "        best_g_loss = G_loss_avg\n",
    "        torch.save(G.state_dict(), os.path.join(save_dir, 'G_best.pth'))\n",
    "        torch.save(D.state_dict(), os.path.join(save_dir, 'D_best.pth'))\n",
    "        print(f\"Saved Best Models at epoch {epoch} | G_loss: {best_g_loss:.4f}\")\n",
    "\n",
    "    save_sample(G, epoch, mb_size, Z_dim)\n",
    "\n",
    "\n",
    "# Inference    \n",
    "# G.load_state_dict(torch.load('checkpoints/G_best.pth'))\n",
    "# G.eval()\n",
    "\n",
    "# save_sample(G, \"best\", mb_size, Z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344b39cc-bbcf-4df6-994c-fd1e40486551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>D_loss</td><td>▁▁▁▂▂▄▅▆▇█</td></tr><tr><td>D_loss_fake</td><td>▁▁▁▂▂▄▅▆▇█</td></tr><tr><td>D_loss_real</td><td>▂▁▁▂▃▄▅▆▇█</td></tr><tr><td>G_loss</td><td>▇█▇█▇▅▃▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>D_loss</td><td>0.72933</td></tr><tr><td>D_loss_fake</td><td>0.33535</td></tr><tr><td>D_loss_real</td><td>0.39398</td></tr><tr><td>G_loss</td><td>2.91352</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">light-astromech-18</strong> at: <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/cp3m6oea' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/cp3m6oea</a><br> View project at: <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_012345-cp3m6oea/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/lab02_stefan/wandb/run-20250506_153234-pzcc0y3o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/pzcc0y3o' target=\"_blank\">smart-cloud-20</a></strong> to <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/pzcc0y3o' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/pzcc0y3o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 72.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0; D_loss: 0.0894; G_loss: 6.3074\n",
      "Saved Best Models at epoch 0 | G_loss: 6.3074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 76.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1; D_loss: 0.0340; G_loss: 6.5768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2; D_loss: 0.0714; G_loss: 4.8203\n",
      "Saved Best Models at epoch 2 | G_loss: 4.8203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3; D_loss: 0.1016; G_loss: 5.4058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4; D_loss: 0.2056; G_loss: 4.8641\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "mb_size = 64\n",
    "Z_dim = 1000\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "\n",
    "# Load MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the 28x28 image to 784\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../MNIST', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=mb_size, shuffle=True)\n",
    "\n",
    "X_dim = 784  # 28 x 28\n",
    "\n",
    "# Xavier Initialization\n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, h_dim, x_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, x_dim)\n",
    "        self.apply(xavier_init)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        out = torch.sigmoid(self.fc2(h))\n",
    "        return out\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, 1)\n",
    "        self.apply(xavier_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        out = self.fc2(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "def logisticGANTraining(G, D, loss_fn, train_loader):\n",
    "    G.train()\n",
    "    D.train()\n",
    "\n",
    "    D_loss_real_total = 0\n",
    "    D_loss_fake_total = 0\n",
    "    G_loss_total = 0\n",
    "    t = tqdm.tqdm(train_loader)\n",
    "    \n",
    "    for it, (X_real, labels) in enumerate(t):\n",
    "        # Prepare real data\n",
    "        X_real = X_real.float().to(device)\n",
    "\n",
    "        # Sample noise and labels\n",
    "        z = torch.randn(X_real.size(0), Z_dim).to(device)\n",
    "        ones_label = torch.ones(X_real.size(0), 1).to(device)\n",
    "        zeros_label = torch.zeros(X_real.size(0), 1).to(device)\n",
    "\n",
    "        # ================= Train Discriminator =================\n",
    "        G_sample = G(z)\n",
    "        D_real = D(X_real)\n",
    "        D_fake = D(G_sample.detach())\n",
    "\n",
    "        D_loss_real = F.softplus(-D_real).mean()  # log(1 + e^{-D(x_real)})\n",
    "        D_loss_fake = F.softplus(D_fake).mean()   # log(1 + e^{D(x_fake)})\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        D_loss_real_total += D_loss_real.item()\n",
    "        D_loss_fake_total += D_loss_fake.item()\n",
    "\n",
    "        D_solver.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_solver.step()\n",
    "\n",
    "        # ================= Train Generator ====================\n",
    "        z = torch.randn(X_real.size(0), Z_dim).to(device)\n",
    "        G_sample = G(z)\n",
    "        D_fake = D(G_sample)\n",
    "\n",
    "        G_loss = F.softplus(-D_fake).mean()  # log(1 + e^{-D(G(z))})\n",
    "        G_loss_total += G_loss.item()\n",
    "\n",
    "        G_solver.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_solver.step()\n",
    "\n",
    "    # ================= Logging =================\n",
    "    D_loss_real_avg = D_loss_real_total / len(train_loader)\n",
    "    D_loss_fake_avg = D_loss_fake_total / len(train_loader)\n",
    "    D_loss_avg = D_loss_real_avg + D_loss_fake_avg\n",
    "    G_loss_avg = G_loss_total / len(train_loader)\n",
    "\n",
    "    wandb.log({\n",
    "        \"D_loss_real\": D_loss_real_avg,\n",
    "        \"D_loss_fake\": D_loss_fake_avg,\n",
    "        \"D_loss\": D_loss_avg,\n",
    "        \"G_loss\": G_loss_avg\n",
    "    })\n",
    "\n",
    "    return G, D, G_loss_avg, D_loss_avg\n",
    "    \n",
    "\n",
    "\n",
    "def save_sample(G, epoch, mb_size, Z_dim):\n",
    "    out_dir = \"out_vanila_GAN2\"\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(mb_size, Z_dim).to(device)\n",
    "        samples = G(z).detach().cpu().numpy()[:16]\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    if not os.path.exists(f'{out_dir}'):\n",
    "        os.makedirs(f'{out_dir}')\n",
    "\n",
    "    plt.savefig(f'{out_dir}/{str(epoch).zfill(3)}.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "########################### Main #######################################\n",
    "wandb_log = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate models\n",
    "G = Generator(Z_dim, h_dim, X_dim).to(device)\n",
    "D = Discriminator(X_dim, h_dim).to(device)\n",
    "\n",
    "# Optimizers\n",
    "G_solver = optim.Adam(G.parameters(), lr=lr)\n",
    "D_solver = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "#def my_bce_loss(preds, targets):\n",
    "#    return F.binary_cross_entropy(preds, targets)\n",
    "\n",
    "#loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn = None\n",
    "\n",
    "if wandb_log: \n",
    "    wandb.init(project=\"conditional-gan-mnist\")\n",
    "\n",
    "    # Log hyperparameters\n",
    "    wandb.config.update({\n",
    "        \"batch_size\": mb_size,\n",
    "        \"Z_dim\": Z_dim,\n",
    "        \"X_dim\": X_dim,\n",
    "        \"h_dim\": h_dim,\n",
    "        \"lr\": lr,\n",
    "    })\n",
    "\n",
    "best_g_loss = float('inf')  # Initialize best generator loss\n",
    "save_dir = 'checkpoints'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#Train epochs\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    G, D, G_loss_avg, D_loss_avg= logisticGANTraining(G, D, loss_fn, train_loader)\n",
    "\n",
    "    print(f'epoch{epoch}; D_loss: {D_loss_avg:.4f}; G_loss: {G_loss_avg:.4f}')\n",
    "\n",
    "    if G_loss_avg < best_g_loss:\n",
    "        best_g_loss = G_loss_avg\n",
    "        torch.save(G.state_dict(), os.path.join(save_dir, 'G_best.pth'))\n",
    "        torch.save(D.state_dict(), os.path.join(save_dir, 'D_best.pth'))\n",
    "        print(f\"Saved Best Models at epoch {epoch} | G_loss: {best_g_loss:.4f}\")\n",
    "\n",
    "    save_sample(G, epoch, mb_size, Z_dim)\n",
    "\n",
    "\n",
    "# Inference    \n",
    "# G.load_state_dict(torch.load('checkpoints/G_best.pth'))\n",
    "# G.eval()\n",
    "\n",
    "# save_sample(G, \"best\", mb_size, Z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "932e3d1a-47a4-4f48-9633-e30a44442c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>D_loss</td><td>▃▁▃▄█</td></tr><tr><td>D_loss_fake</td><td>▂▁▃▄█</td></tr><tr><td>D_loss_real</td><td>▄▁▂▄█</td></tr><tr><td>G_loss</td><td>▇█▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>D_loss</td><td>0.20555</td></tr><tr><td>D_loss_fake</td><td>0.08836</td></tr><tr><td>D_loss_real</td><td>0.11719</td></tr><tr><td>G_loss</td><td>4.86413</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-cloud-20</strong> at: <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/pzcc0y3o' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/pzcc0y3o</a><br> View project at: <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_153234-pzcc0y3o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/lab02_stefan/wandb/run-20250506_153402-4k7m29yn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/4k7m29yn' target=\"_blank\">confused-firefly-21</a></strong> to <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/4k7m29yn' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/4k7m29yn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0; D_loss: 0.0710; G_loss: 6.3610\n",
      "Saved Best Models at epoch 0 | G_loss: 6.3610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 72.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1; D_loss: 0.0345; G_loss: 6.1787\n",
      "Saved Best Models at epoch 1 | G_loss: 6.1787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2; D_loss: 0.0462; G_loss: 5.3850\n",
      "Saved Best Models at epoch 2 | G_loss: 5.3850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3; D_loss: 0.0906; G_loss: 5.9882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 75.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4; D_loss: 0.1829; G_loss: 5.0895\n",
      "Saved Best Models at epoch 4 | G_loss: 5.0895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5; D_loss: 0.3064; G_loss: 4.4196\n",
      "Saved Best Models at epoch 5 | G_loss: 4.4196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6; D_loss: 0.4397; G_loss: 3.8090\n",
      "Saved Best Models at epoch 6 | G_loss: 3.8090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 75.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7; D_loss: 0.5412; G_loss: 3.3873\n",
      "Saved Best Models at epoch 7 | G_loss: 3.3873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8; D_loss: 0.5925; G_loss: 3.1010\n",
      "Saved Best Models at epoch 8 | G_loss: 3.1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9; D_loss: 0.6432; G_loss: 2.8428\n",
      "Saved Best Models at epoch 9 | G_loss: 2.8428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10; D_loss: 0.6640; G_loss: 2.7327\n",
      "Saved Best Models at epoch 10 | G_loss: 2.7327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch11; D_loss: 0.6983; G_loss: 2.5987\n",
      "Saved Best Models at epoch 11 | G_loss: 2.5987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch12; D_loss: 0.7276; G_loss: 2.4775\n",
      "Saved Best Models at epoch 12 | G_loss: 2.4775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch13; D_loss: 0.7618; G_loss: 2.4057\n",
      "Saved Best Models at epoch 13 | G_loss: 2.4057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch14; D_loss: 0.7581; G_loss: 2.3252\n",
      "Saved Best Models at epoch 14 | G_loss: 2.3252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 75.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch15; D_loss: 0.7851; G_loss: 2.1577\n",
      "Saved Best Models at epoch 15 | G_loss: 2.1577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 76.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch16; D_loss: 0.7700; G_loss: 2.1157\n",
      "Saved Best Models at epoch 16 | G_loss: 2.1157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 86.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch17; D_loss: 0.7688; G_loss: 2.1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 75.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch18; D_loss: 0.7694; G_loss: 2.1303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 87.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch19; D_loss: 0.7870; G_loss: 2.1314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch20; D_loss: 0.7903; G_loss: 2.0715\n",
      "Saved Best Models at epoch 20 | G_loss: 2.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 75.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch21; D_loss: 0.8048; G_loss: 2.0142\n",
      "Saved Best Models at epoch 21 | G_loss: 2.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:11<00:00, 79.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch22; D_loss: 0.8009; G_loss: 2.0593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:11<00:00, 84.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch23; D_loss: 0.8064; G_loss: 2.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:11<00:00, 79.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch24; D_loss: 0.7939; G_loss: 2.0240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 75.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch25; D_loss: 0.7970; G_loss: 1.9613\n",
      "Saved Best Models at epoch 25 | G_loss: 1.9613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 75.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch26; D_loss: 0.7880; G_loss: 1.9552\n",
      "Saved Best Models at epoch 26 | G_loss: 1.9552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch27; D_loss: 0.7858; G_loss: 2.0548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch28; D_loss: 0.7847; G_loss: 2.0497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch29; D_loss: 0.7877; G_loss: 1.9943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch30; D_loss: 0.7793; G_loss: 1.9952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch31; D_loss: 0.7757; G_loss: 2.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch32; D_loss: 0.7658; G_loss: 2.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 75.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch33; D_loss: 0.7579; G_loss: 2.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 75.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch34; D_loss: 0.7440; G_loss: 2.1255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch35; D_loss: 0.7376; G_loss: 2.1245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 75.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch36; D_loss: 0.7306; G_loss: 2.1425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 75.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch37; D_loss: 0.7210; G_loss: 2.1740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:11<00:00, 82.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch38; D_loss: 0.7085; G_loss: 2.2152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch39; D_loss: 0.7003; G_loss: 2.2271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 77.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch40; D_loss: 0.6907; G_loss: 2.2522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:09<00:00, 98.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch41; D_loss: 0.6904; G_loss: 2.2826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch42; D_loss: 0.6802; G_loss: 2.3431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch43; D_loss: 0.6715; G_loss: 2.3494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 75.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch44; D_loss: 0.6676; G_loss: 2.4003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch45; D_loss: 0.6595; G_loss: 2.4485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch46; D_loss: 0.6498; G_loss: 2.4560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch47; D_loss: 0.6445; G_loss: 2.4703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch48; D_loss: 0.6430; G_loss: 2.4832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 91.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch49; D_loss: 0.6377; G_loss: 2.5040\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "mb_size = 64\n",
    "Z_dim = 1000\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "\n",
    "# Load MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the 28x28 image to 784\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../MNIST', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=mb_size, shuffle=True)\n",
    "\n",
    "X_dim = 784  # 28 x 28\n",
    "\n",
    "# Xavier Initialization\n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, h_dim, x_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, x_dim)\n",
    "        self.apply(xavier_init)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        out = torch.sigmoid(self.fc2(h))\n",
    "        return out\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, 1)\n",
    "        self.apply(xavier_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        out = self.fc2(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "def logisticGANTraining(G, D, loss_fn, train_loader):\n",
    "    G.train()\n",
    "    D.train()\n",
    "\n",
    "    D_loss_real_total = 0\n",
    "    D_loss_fake_total = 0\n",
    "    G_loss_total = 0\n",
    "    t = tqdm.tqdm(train_loader)\n",
    "    \n",
    "    for it, (X_real, labels) in enumerate(t):\n",
    "        # Prepare real data\n",
    "        X_real = X_real.float().to(device)\n",
    "\n",
    "        # Sample noise and labels\n",
    "        z = torch.randn(X_real.size(0), Z_dim).to(device)\n",
    "        ones_label = torch.ones(X_real.size(0), 1).to(device)\n",
    "        zeros_label = torch.zeros(X_real.size(0), 1).to(device)\n",
    "\n",
    "        # ================= Train Discriminator =================\n",
    "        G_sample = G(z)\n",
    "        D_real = D(X_real)\n",
    "        D_fake = D(G_sample.detach())\n",
    "\n",
    "        D_loss_real = F.softplus(-D_real).mean()  # log(1 + e^{-D(x_real)})\n",
    "        D_loss_fake = F.softplus(D_fake).mean()   # log(1 + e^{D(x_fake)})\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        D_loss_real_total += D_loss_real.item()\n",
    "        D_loss_fake_total += D_loss_fake.item()\n",
    "\n",
    "        D_solver.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_solver.step()\n",
    "\n",
    "        # ================= Train Generator ====================\n",
    "        z = torch.randn(X_real.size(0), Z_dim).to(device)\n",
    "        G_sample = G(z)\n",
    "        D_fake = D(G_sample)\n",
    "\n",
    "        G_loss = F.softplus(-D_fake).mean()  # log(1 + e^{-D(G(z))})\n",
    "        G_loss_total += G_loss.item()\n",
    "\n",
    "        G_solver.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_solver.step()\n",
    "\n",
    "    # ================= Logging =================\n",
    "    D_loss_real_avg = D_loss_real_total / len(train_loader)\n",
    "    D_loss_fake_avg = D_loss_fake_total / len(train_loader)\n",
    "    D_loss_avg = D_loss_real_avg + D_loss_fake_avg\n",
    "    G_loss_avg = G_loss_total / len(train_loader)\n",
    "\n",
    "    wandb.log({\n",
    "        \"D_loss_real\": D_loss_real_avg,\n",
    "        \"D_loss_fake\": D_loss_fake_avg,\n",
    "        \"D_loss\": D_loss_avg,\n",
    "        \"G_loss\": G_loss_avg\n",
    "    })\n",
    "\n",
    "    return G, D, G_loss_avg, D_loss_avg\n",
    "    \n",
    "\n",
    "\n",
    "def save_sample(G, epoch, mb_size, Z_dim):\n",
    "    out_dir = \"out_vanila_GAN2\"\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(mb_size, Z_dim).to(device)\n",
    "        samples = G(z).detach().cpu().numpy()[:16]\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    if not os.path.exists(f'{out_dir}'):\n",
    "        os.makedirs(f'{out_dir}')\n",
    "\n",
    "    plt.savefig(f'{out_dir}/{str(epoch).zfill(3)}.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "########################### Main #######################################\n",
    "wandb_log = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate models\n",
    "G = Generator(Z_dim, h_dim, X_dim).to(device)\n",
    "D = Discriminator(X_dim, h_dim).to(device)\n",
    "\n",
    "# Optimizers\n",
    "G_solver = optim.Adam(G.parameters(), lr=lr)\n",
    "D_solver = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "#def my_bce_loss(preds, targets):\n",
    "#    return F.binary_cross_entropy(preds, targets)\n",
    "\n",
    "#loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn = None\n",
    "\n",
    "if wandb_log: \n",
    "    wandb.init(project=\"conditional-gan-mnist\")\n",
    "\n",
    "    # Log hyperparameters\n",
    "    wandb.config.update({\n",
    "        \"batch_size\": mb_size,\n",
    "        \"Z_dim\": Z_dim,\n",
    "        \"X_dim\": X_dim,\n",
    "        \"h_dim\": h_dim,\n",
    "        \"lr\": lr,\n",
    "    })\n",
    "\n",
    "best_g_loss = float('inf')  # Initialize best generator loss\n",
    "save_dir = 'checkpoints'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#Train epochs\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    G, D, G_loss_avg, D_loss_avg= logisticGANTraining(G, D, loss_fn, train_loader)\n",
    "\n",
    "    print(f'epoch{epoch}; D_loss: {D_loss_avg:.4f}; G_loss: {G_loss_avg:.4f}')\n",
    "\n",
    "    if G_loss_avg < best_g_loss:\n",
    "        best_g_loss = G_loss_avg\n",
    "        torch.save(G.state_dict(), os.path.join(save_dir, 'G_best.pth'))\n",
    "        torch.save(D.state_dict(), os.path.join(save_dir, 'D_best.pth'))\n",
    "        print(f\"Saved Best Models at epoch {epoch} | G_loss: {best_g_loss:.4f}\")\n",
    "\n",
    "    save_sample(G, epoch, mb_size, Z_dim)\n",
    "\n",
    "\n",
    "# Inference    \n",
    "# G.load_state_dict(torch.load('checkpoints/G_best.pth'))\n",
    "# G.eval()\n",
    "\n",
    "# save_sample(G, \"best\", mb_size, Z_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a727423-2d78-4f2c-8d2f-baf279f63768",
   "metadata": {},
   "source": [
    "In comparing Binary Cross Entropy (Task 1) and Logistic Loss (Task 2) for 10 epochs, both led to stable and successful GAN training. Logistic loss showed smoother discriminator behaviour and more gradual G_loss drop, while BCE had slightly faster convergence early on. Final Generator performance was comparable, with both reaching G_loss values around 2.7–2.9 by epoch 9. Logistic loss appears more robust and theoretically stable, while BCE works well in practice and may converge faster with fewer epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
