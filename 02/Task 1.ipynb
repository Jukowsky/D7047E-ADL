{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0af60ce-b191-4ea2-a364-c149a3c6e04d",
   "metadata": {},
   "source": [
    "Task 1 \n",
    "We have given a vanilla GAN PyTorch implementation in this lab files. To understand how\n",
    "it works, you can refer the Vanilla GAN as described by Goodfellow in his paper and the\n",
    "assosiated GitHub. Further, you can read this article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a8fd08-4ea6-49ab-bc61-b9765827a1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>D_loss</td><td>▄▁▁▃█</td></tr><tr><td>D_loss_fake</td><td>▃▁▁▃█</td></tr><tr><td>D_loss_real</td><td>▄▁▁▃█</td></tr><tr><td>G_loss</td><td>█▇▇▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>D_loss</td><td>0.1409</td></tr><tr><td>D_loss_fake</td><td>0.05926</td></tr><tr><td>D_loss_real</td><td>0.08164</td></tr><tr><td>G_loss</td><td>5.41064</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hokey-nexu-16</strong> at: <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/nb1n5rcv' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/nb1n5rcv</a><br> View project at: <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_010749-nb1n5rcv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/lab02_stefan/wandb/run-20250505_012600-gmpy5mvb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/gmpy5mvb' target=\"_blank\">holographic-trooper-19</a></strong> to <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/gmpy5mvb' target=\"_blank\">https://wandb.ai/canatilgan-lule-university-of-technology/conditional-gan-mnist/runs/gmpy5mvb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 72.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0; D_loss: 0.0754; G_loss: 6.2475\n",
      "Saved Best Models at epoch 0 | G_loss: 6.2475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:08<00:00, 104.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1; D_loss: 0.0366; G_loss: 6.0632\n",
      "Saved Best Models at epoch 1 | G_loss: 6.0632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 91.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2; D_loss: 0.0437; G_loss: 5.3029\n",
      "Saved Best Models at epoch 2 | G_loss: 5.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3; D_loss: 0.0847; G_loss: 5.4559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4; D_loss: 0.1988; G_loss: 4.8418\n",
      "Saved Best Models at epoch 4 | G_loss: 4.8418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5; D_loss: 0.3419; G_loss: 4.1329\n",
      "Saved Best Models at epoch 5 | G_loss: 4.1329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6; D_loss: 0.3971; G_loss: 3.6860\n",
      "Saved Best Models at epoch 6 | G_loss: 3.6860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7; D_loss: 0.4568; G_loss: 3.3866\n",
      "Saved Best Models at epoch 7 | G_loss: 3.3866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 72.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8; D_loss: 0.5087; G_loss: 3.2097\n",
      "Saved Best Models at epoch 8 | G_loss: 3.2097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 74.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9; D_loss: 0.5714; G_loss: 2.7413\n",
      "Saved Best Models at epoch 9 | G_loss: 2.7413\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "mb_size = 64\n",
    "Z_dim = 1000\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "\n",
    "# Load MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the 28x28 image to 784\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../MNIST', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=mb_size, shuffle=True)\n",
    "\n",
    "X_dim = 784  # 28 x 28\n",
    "\n",
    "# Xavier Initialization\n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, h_dim, x_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, x_dim)\n",
    "        self.apply(xavier_init)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        out = torch.sigmoid(self.fc2(h))\n",
    "        return out\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, 1)\n",
    "        self.apply(xavier_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        out = torch.sigmoid(self.fc2(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "def cGANTraining(G, D, loss_fn, train_loader):\n",
    "    G.train()\n",
    "    D.train()\n",
    "\n",
    "    D_loss_real_total = 0\n",
    "    D_loss_fake_total = 0\n",
    "    G_loss_total = 0\n",
    "    t = tqdm.tqdm(train_loader)\n",
    "    \n",
    "    for it, (X_real, labels) in enumerate(t):\n",
    "        # Prepare real data\n",
    "        X_real = X_real.float().to(device)\n",
    "\n",
    "        # Sample noise and labels\n",
    "        z = torch.randn(X_real.size(0), Z_dim).to(device)\n",
    "        ones_label = torch.ones(X_real.size(0), 1).to(device)\n",
    "        zeros_label = torch.zeros(X_real.size(0), 1).to(device)\n",
    "\n",
    "        # ================= Train Discriminator =================\n",
    "        G_sample = G(z)\n",
    "        D_real = D(X_real)\n",
    "        D_fake = D(G_sample.detach())\n",
    "\n",
    "        D_loss_real = loss_fn(D_real, ones_label)\n",
    "        D_loss_fake = loss_fn(D_fake, zeros_label)\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        D_loss_real_total += D_loss_real.item()\n",
    "        D_loss_fake_total += D_loss_fake.item()\n",
    "\n",
    "        D_solver.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_solver.step()\n",
    "\n",
    "        # ================= Train Generator ====================\n",
    "        z = torch.randn(X_real.size(0), Z_dim).to(device)\n",
    "        G_sample = G(z)\n",
    "        D_fake = D(G_sample)\n",
    "\n",
    "        G_loss = loss_fn(D_fake, ones_label)\n",
    "        G_loss_total += G_loss.item()\n",
    "\n",
    "        G_solver.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_solver.step()\n",
    "\n",
    "    # ================= Logging =================\n",
    "    D_loss_real_avg = D_loss_real_total / len(train_loader)\n",
    "    D_loss_fake_avg = D_loss_fake_total / len(train_loader)\n",
    "    D_loss_avg = D_loss_real_avg + D_loss_fake_avg\n",
    "    G_loss_avg = G_loss_total / len(train_loader)\n",
    "\n",
    "    wandb.log({\n",
    "        \"D_loss_real\": D_loss_real_avg,\n",
    "        \"D_loss_fake\": D_loss_fake_avg,\n",
    "        \"D_loss\": D_loss_avg,\n",
    "        \"G_loss\": G_loss_avg\n",
    "    })\n",
    "\n",
    "    return G, D, G_loss_avg, D_loss_avg\n",
    "    \n",
    "\n",
    "\n",
    "def save_sample(G, epoch, mb_size, Z_dim):\n",
    "    out_dir = \"out_vanila_GAN2\"\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(mb_size, Z_dim).to(device)\n",
    "        samples = G(z).detach().cpu().numpy()[:16]\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    if not os.path.exists(f'{out_dir}'):\n",
    "        os.makedirs(f'{out_dir}')\n",
    "\n",
    "    plt.savefig(f'{out_dir}/{str(epoch).zfill(3)}.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "########################### Main #######################################\n",
    "wandb_log = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate models\n",
    "G = Generator(Z_dim, h_dim, X_dim).to(device)\n",
    "D = Discriminator(X_dim, h_dim).to(device)\n",
    "\n",
    "# Optimizers\n",
    "G_solver = optim.Adam(G.parameters(), lr=lr)\n",
    "D_solver = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "def my_bce_loss(preds, targets):\n",
    "    return F.binary_cross_entropy(preds, targets)\n",
    "\n",
    "#loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn = my_bce_loss\n",
    "\n",
    "if wandb_log: \n",
    "    wandb.init(project=\"conditional-gan-mnist\")\n",
    "\n",
    "    # Log hyperparameters\n",
    "    wandb.config.update({\n",
    "        \"batch_size\": mb_size,\n",
    "        \"Z_dim\": Z_dim,\n",
    "        \"X_dim\": X_dim,\n",
    "        \"h_dim\": h_dim,\n",
    "        \"lr\": lr,\n",
    "    })\n",
    "\n",
    "best_g_loss = float('inf')  # Initialize best generator loss\n",
    "save_dir = 'checkpoints'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#Train epochs\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    G, D, G_loss_avg, D_loss_avg= cGANTraining(G, D, loss_fn, train_loader)\n",
    "\n",
    "    print(f'epoch{epoch}; D_loss: {D_loss_avg:.4f}; G_loss: {G_loss_avg:.4f}')\n",
    "\n",
    "    if G_loss_avg < best_g_loss:\n",
    "        best_g_loss = G_loss_avg\n",
    "        torch.save(G.state_dict(), os.path.join(save_dir, 'G_best.pth'))\n",
    "        torch.save(D.state_dict(), os.path.join(save_dir, 'D_best.pth'))\n",
    "        print(f\"Saved Best Models at epoch {epoch} | G_loss: {best_g_loss:.4f}\")\n",
    "\n",
    "    save_sample(G, epoch, mb_size, Z_dim)\n",
    "\n",
    "\n",
    "# Inference    \n",
    "# G.load_state_dict(torch.load('checkpoints/G_best.pth'))\n",
    "# G.eval()\n",
    "\n",
    "# save_sample(G, \"best\", mb_size, Z_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
